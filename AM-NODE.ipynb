{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ComponentArrays, Lux, DiffEqFlux, OrdinaryDiffEq, Optimization, OptimizationOptimJL,\n",
    "      OptimizationOptimisers, Random, Plots\n",
    "using ArrheniusModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "arrhenius_rate (generic function with 2 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function arrhenius_rate(pe::PhaseEnergies, T=300)\n",
    "    kb = 8.617e-5 #eV/K\n",
    "    A = 1.0 # Arrhenius prefactor\n",
    "    pe.K = A * exp.(-pe.Ea_plus_ΔG ./ (kb * T))\n",
    "    # Adjust the diagonal elements\n",
    "    for i in 1:size(pe.K, 1)\n",
    "        pe.K[i, i] =  -1 * sum(pe.K[i, [1:i-1; i+1:end]])\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21×3×21 Array{Float64, 3}:\n",
       "[:, :, 1] =\n",
       " 1.0  0.0  0.0\n",
       " 0.0  0.0  0.0\n",
       " 0.0  0.0  0.0\n",
       " 0.0  0.0  0.0\n",
       " 0.0  0.0  0.0\n",
       " 0.0  0.0  0.0\n",
       " 0.0  0.0  0.0\n",
       " 0.0  0.0  0.0\n",
       " 0.0  0.0  0.0\n",
       " 0.0  0.0  0.0\n",
       " ⋮         \n",
       " 0.0  0.0  0.0\n",
       " 0.0  0.0  0.0\n",
       " 0.0  0.0  0.0\n",
       " 0.0  0.0  0.0\n",
       " 0.0  0.0  0.0\n",
       " 0.0  0.0  0.0\n",
       " 0.0  0.0  0.0\n",
       " 0.0  0.0  0.0\n",
       " 0.0  0.0  0.0\n",
       "\n",
       "[:, :, 2] =\n",
       " 0.714042      7.20251e-18   0.285958\n",
       " 1.0          -9.76963e-49  -4.1859e-32\n",
       " 3.64848e-30   0.0           0.0\n",
       " 0.0           0.0           0.0\n",
       " 0.0           0.0           0.0\n",
       " 0.0           0.0           0.0\n",
       " 0.0           0.0           0.0\n",
       " 0.0           0.0           0.0\n",
       " 0.0           0.0           0.0\n",
       " 0.0           0.0           0.0\n",
       " ⋮                          \n",
       " 0.0           0.0           0.0\n",
       " 0.0           0.0           0.0\n",
       " 0.0           0.0           0.0\n",
       " 0.0           0.0           0.0\n",
       " 0.0           0.0           0.0\n",
       " 0.0           0.0           0.0\n",
       " 0.0           0.0           0.0\n",
       " 0.0           0.0           0.0\n",
       " 0.0           0.0           0.0\n",
       "\n",
       "[:, :, 3] =\n",
       " 0.521676   1.31426e-17   0.478324\n",
       " 0.737811   6.54736e-18   0.262189\n",
       " 1.0       -4.57584e-33  -1.96057e-16\n",
       " 1.0        0.0           0.0\n",
       " 0.0        0.0           0.0\n",
       " 0.0        0.0           0.0\n",
       " 0.0        0.0           0.0\n",
       " 0.0        0.0           0.0\n",
       " 0.0        0.0           0.0\n",
       " 0.0        0.0           0.0\n",
       " ⋮                       \n",
       " 0.0        0.0           0.0\n",
       " 0.0        0.0           0.0\n",
       " 0.0        0.0           0.0\n",
       " 0.0        0.0           0.0\n",
       " 0.0        0.0           0.0\n",
       " 0.0        0.0           0.0\n",
       " 0.0        0.0           0.0\n",
       " 0.0        0.0           0.0\n",
       " 0.0        0.0           0.0\n",
       "\n",
       ";;; … \n",
       "\n",
       "[:, :, 19] =\n",
       " 0.126986      7.19896e-17   0.873014\n",
       " 0.127367      6.83049e-17   0.872633\n",
       " 0.127828      6.52881e-17   0.872172\n",
       " 0.128576      6.19329e-17   0.871424\n",
       " 0.129687      5.85728e-17   0.870313\n",
       " 0.131544      5.48681e-17   0.868456\n",
       " 0.134099      5.14884e-17   0.865901\n",
       " 0.137898      4.8092e-17    0.862102\n",
       " 0.143545      4.46706e-17   0.856455\n",
       " 0.151939      4.12122e-17   0.848061\n",
       " ⋮                          \n",
       " 0.210538      3.0386e-17    0.789462\n",
       " 0.251526      2.64879e-17   0.748474\n",
       " 0.312455      2.23208e-17   0.687545\n",
       " 0.403027      1.77538e-17   0.596973\n",
       " 0.537666      1.25924e-17   0.462334\n",
       " 0.737811      6.54736e-18   0.262189\n",
       " 1.0          -1.60066e-44  -6.85818e-28\n",
       " 6.05845e-26   0.0           0.0\n",
       " 0.0           0.0           0.0\n",
       "\n",
       "[:, :, 20] =\n",
       " 0.126759  7.53378e-17  0.873241\n",
       " 0.127014  7.16547e-17  0.872986\n",
       " 0.127325  6.864e-17    0.872675\n",
       " 0.127828  6.52881e-17  0.872172\n",
       " 0.128576  6.19329e-17  0.871424\n",
       " 0.129824  5.82364e-17  0.870176\n",
       " 0.131544  5.48681e-17  0.868456\n",
       " 0.134099  5.14884e-17  0.865901\n",
       " 0.137898  4.8092e-17   0.862102\n",
       " 0.143545  4.46706e-17  0.856455\n",
       " ⋮                      \n",
       " 0.182965  3.41032e-17  0.817035\n",
       " 0.210538  3.0386e-17   0.789462\n",
       " 0.251526  2.64879e-17  0.748474\n",
       " 0.312455  2.23208e-17  0.687545\n",
       " 0.403027  1.77538e-17  0.596973\n",
       " 0.537666  1.25924e-17  0.462334\n",
       " 0.737811  6.54736e-18  0.262189\n",
       " 1.0       0.0          0.0\n",
       " 1.0       0.0          0.0\n",
       "\n",
       "[:, :, 21] =\n",
       " 0.126618  7.83502e-17  0.873382\n",
       " 0.126797  7.46682e-17  0.873203\n",
       " 0.127014  7.16547e-17  0.872986\n",
       " 0.127367  6.83049e-17  0.872633\n",
       " 0.12789   6.49527e-17  0.87211\n",
       " 0.128764  6.12613e-17  0.871236\n",
       " 0.129967  5.78999e-17  0.870033\n",
       " 0.131756  5.45307e-17  0.868244\n",
       " 0.134415  5.11496e-17  0.865585\n",
       " 0.138367  4.77511e-17  0.861633\n",
       " ⋮                      \n",
       " 0.165958  3.73433e-17  0.834042\n",
       " 0.185257  3.37378e-17  0.814743\n",
       " 0.213945  3.00056e-17  0.786055\n",
       " 0.25659   2.60851e-17  0.74341\n",
       " 0.319983  2.18847e-17  0.680017\n",
       " 0.414218  1.72683e-17  0.585782\n",
       " 0.554302  1.20334e-17  0.445698\n",
       " 0.737811  6.54736e-18  0.262189\n",
       " 1.0       0.0          0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rng = Xoshiro(0)\n",
    "G = [-5.92, -5.942, -5.97]\n",
    "Ea = [0.00 1.00 0.01; 1.00 0.00 1.00; 0.01 1.00 0.00]\n",
    "pe = PhaseEnergies(G, Ea)\n",
    "T = 300.0\n",
    "t= 10\n",
    "dt = 0.05\n",
    "num_steps = floor(Int, t/dt)\n",
    "num_layers = floor(Int, t/0.5)+1\n",
    "flow_rate = 0.5\n",
    "decay_coefficient = 0.00001 * flow_rate\n",
    "fcoeff = flow_coefficient(\"exponential\", num_layers, decay_coefficient)\n",
    "\n",
    "function deposition_rates!(dc, c, p, t)\n",
    "    # Unpack parameters\n",
    "    fcoeff, pe, j0, j, dt, num_steps, num_layers = p\n",
    "    # Calculate deposition rates\n",
    "    j = floor(Int, t / 0.5) + 1\n",
    "    f = reverse(fcoeff[j: num_layers+j-1])\n",
    "    dc .= c .* f * pe.K\n",
    "    if j != j0\n",
    "        c[j+1, 1] = 1.0\n",
    "        j = j0\n",
    "    end\n",
    "end\n",
    "\n",
    "n = n_phases(pe)\n",
    "c0 = zeros(num_layers, n)\n",
    "c0[1, 1] = 1.0\n",
    "arrhenius_rate(pe, T)\n",
    "j = 0\n",
    "j0 = 0\n",
    "p = (fcoeff, pe, j0, j, dt, num_steps, num_layers)\n",
    "tspan = (0.0, (num_steps-1) * dt)\n",
    "prob = ODEProblem(deposition_rates!, c0, tspan, p)\n",
    "ode_data = Array(solve(prob, Euler(), saveat = 0.5, dt = dt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(ode_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralODE(\n",
       "    model = Chain(\n",
       "        layer_1 = BatchNorm(2, affine=true, track_stats=true),  \u001b[90m# 4 parameters\u001b[39m\u001b[90m, plus 5\u001b[39m\n",
       "        layer_2 = Dense(2 => 21, relu),  \u001b[90m# 63 parameters\u001b[39m\n",
       "        layer_3 = Dense(21 => 63, relu),  \u001b[90m# 1_386 parameters\u001b[39m\n",
       "        layer_4 = Dense(63 => 51),      \u001b[90m# 3_264 parameters\u001b[39m\n",
       "    ),\n",
       ") \u001b[90m        # Total: \u001b[39m4_717 parameters,\n",
       "\u001b[90m          #        plus \u001b[39m5 states."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inputs = [T, flow_rate]\n",
    "input_size = length(inputs)  # Replace with the actual size of `inputs` if it's not a 1D vector\n",
    "fcoeff_size = length(fcoeff)\n",
    "output_size = n ^ 2 + fcoeff_size + 1\n",
    "dudt2 = Chain(\n",
    "    BatchNorm(input_size),\n",
    "    Dense(2, 21, relu),  # First layer, adjust `input_size` accordingly\n",
    "    Dense(21, 63, relu),         # Intermediate layer\n",
    "    Dense(63, output_size)       # Output layer, adjust `output_size` accordingly\n",
    ")\n",
    "\n",
    "p, st = Lux.setup(rng, dudt2)\n",
    "prob_neuralode = NeuralODE(dudt2, tspan, Euler(); saveat = 0.5, dt=dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss_neuralode (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function predict_neuralode(p)\n",
    "    Array(prob_neuralode(c0, p, st)[1])\n",
    "end\n",
    "\n",
    "function loss_neuralode(p)\n",
    "    pred = predict_neuralode(p)\n",
    "    loss = sum(abs2, ode_data .- pred)\n",
    "    return loss, pred\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArgumentError",
     "evalue": "ArgumentError: Invalid Dimensions!",
     "output_type": "error",
     "traceback": [
      "ArgumentError: Invalid Dimensions!\n",
      "\n",
      "Stacktrace:\n",
      "  [1] _get_reshape_dims\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\LuxLib\\mwXSu\\src\\utils.jl:9 [inlined]\n",
      "  [2] _reshape_into_proper_shape\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\LuxLib\\mwXSu\\src\\utils.jl:17 [inlined]\n",
      "  [3] __normalization(x::Matrix{Float64}, running_mean::Vector{Float32}, running_var::Vector{Float32}, scale::SubArray{Float32, 1, Vector{Float32}, Tuple{UnitRange{Int64}}, true}, bias::SubArray{Float32, 1, Vector{Float32}, Tuple{UnitRange{Int64}}, true}, reduce_dims::Val{(2,)}, training::Val{true}, momentum::Float32, epsilon::Float32, act::typeof(identity))\n",
      "    @ LuxLib C:\\Users\\heyye\\.julia\\packages\\LuxLib\\mwXSu\\src\\impl\\normalization.jl:61\n",
      "  [4] _normalization\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\LuxLib\\mwXSu\\ext\\LuxLibReverseDiffExt.jl:70 [inlined]\n",
      "  [5] batchnorm\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\LuxLib\\mwXSu\\src\\api\\batchnorm.jl:44 [inlined]\n",
      "  [6] (::BatchNorm{true, true, Float32, typeof(identity), typeof(zeros32), typeof(ones32)})(x::Matrix{Float64}, ps::ComponentVector{Float32, SubArray{Float32, 1, Vector{Float32}, Tuple{UnitRange{Int64}}, true}, Tuple{Axis{(scale = 1:2, bias = 3:4)}}}, st::@NamedTuple{running_mean::Vector{Float32}, running_var::Vector{Float32}, training::Val{true}})\n",
      "    @ Lux C:\\Users\\heyye\\.julia\\packages\\Lux\\LhwgF\\src\\layers\\normalize.jl:137\n",
      "  [7] apply\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\LuxCore\\9qfdM\\src\\LuxCore.jl:168 [inlined]\n",
      "  [8] macro expansion\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\Lux\\LhwgF\\src\\layers\\containers.jl:0 [inlined]\n",
      "  [9] applychain(layers::@NamedTuple{layer_1::BatchNorm{true, true, Float32, typeof(identity), typeof(zeros32), typeof(ones32)}, layer_2::Dense{true, typeof(relu), typeof(glorot_uniform), typeof(zeros32)}, layer_3::Dense{true, typeof(relu), typeof(glorot_uniform), typeof(zeros32)}, layer_4::Dense{true, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}}, x::Matrix{Float64}, ps::ComponentVector{Float32, Vector{Float32}, Tuple{Axis{(layer_1 = ViewAxis(1:4, Axis(scale = 1:2, bias = 3:4)), layer_2 = ViewAxis(5:67, Axis(weight = ViewAxis(1:42, ShapedAxis((21, 2))), bias = ViewAxis(43:63, ShapedAxis((21, 1))))), layer_3 = ViewAxis(68:1453, Axis(weight = ViewAxis(1:1323, ShapedAxis((63, 21))), bias = ViewAxis(1324:1386, ShapedAxis((63, 1))))), layer_4 = ViewAxis(1454:4717, Axis(weight = ViewAxis(1:3213, ShapedAxis((51, 63))), bias = ViewAxis(3214:3264, ShapedAxis((51, 1))))))}}}, st::@NamedTuple{layer_1::@NamedTuple{running_mean::Vector{Float32}, running_var::Vector{Float32}, training::Val{true}}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}})\n",
      "    @ Lux C:\\Users\\heyye\\.julia\\packages\\Lux\\LhwgF\\src\\layers\\containers.jl:498\n",
      " [10] Chain\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\Lux\\LhwgF\\src\\layers\\containers.jl:496 [inlined]\n",
      " [11] apply\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\LuxCore\\9qfdM\\src\\LuxCore.jl:168 [inlined]\n",
      " [12] (::StatefulLuxLayer{true, Chain{@NamedTuple{layer_1::BatchNorm{true, true, Float32, typeof(identity), typeof(zeros32), typeof(ones32)}, layer_2::Dense{true, typeof(relu), typeof(glorot_uniform), typeof(zeros32)}, layer_3::Dense{true, typeof(relu), typeof(glorot_uniform), typeof(zeros32)}, layer_4::Dense{true, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}}}, Nothing, @NamedTuple{layer_1::@NamedTuple{running_mean::Vector{Float32}, running_var::Vector{Float32}, training::Val{true}}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}})(x::Matrix{Float64}, p::ComponentVector{Float32, Vector{Float32}, Tuple{Axis{(layer_1 = ViewAxis(1:4, Axis(scale = 1:2, bias = 3:4)), layer_2 = ViewAxis(5:67, Axis(weight = ViewAxis(1:42, ShapedAxis((21, 2))), bias = ViewAxis(43:63, ShapedAxis((21, 1))))), layer_3 = ViewAxis(68:1453, Axis(weight = ViewAxis(1:1323, ShapedAxis((63, 21))), bias = ViewAxis(1324:1386, ShapedAxis((63, 1))))), layer_4 = ViewAxis(1454:4717, Axis(weight = ViewAxis(1:3213, ShapedAxis((51, 63))), bias = ViewAxis(3214:3264, ShapedAxis((51, 1))))))}}})\n",
      "    @ Lux C:\\Users\\heyye\\.julia\\packages\\Lux\\LhwgF\\src\\helpers\\stateful.jl:112\n",
      " [13] (::DiffEqFlux.var\"#dudt#19\"{StatefulLuxLayer{true, Chain{@NamedTuple{layer_1::BatchNorm{true, true, Float32, typeof(identity), typeof(zeros32), typeof(ones32)}, layer_2::Dense{true, typeof(relu), typeof(glorot_uniform), typeof(zeros32)}, layer_3::Dense{true, typeof(relu), typeof(glorot_uniform), typeof(zeros32)}, layer_4::Dense{true, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}}}, Nothing, @NamedTuple{layer_1::@NamedTuple{running_mean::Vector{Float32}, running_var::Vector{Float32}, training::Val{true}}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}})(u::Matrix{Float64}, p::ComponentVector{Float32, Vector{Float32}, Tuple{Axis{(layer_1 = ViewAxis(1:4, Axis(scale = 1:2, bias = 3:4)), layer_2 = ViewAxis(5:67, Axis(weight = ViewAxis(1:42, ShapedAxis((21, 2))), bias = ViewAxis(43:63, ShapedAxis((21, 1))))), layer_3 = ViewAxis(68:1453, Axis(weight = ViewAxis(1:1323, ShapedAxis((63, 21))), bias = ViewAxis(1324:1386, ShapedAxis((63, 1))))), layer_4 = ViewAxis(1454:4717, Axis(weight = ViewAxis(1:3213, ShapedAxis((51, 63))), bias = ViewAxis(3214:3264, ShapedAxis((51, 1))))))}}}, t::Float64)\n",
      "    @ DiffEqFlux C:\\Users\\heyye\\.julia\\packages\\DiffEqFlux\\TglmB\\src\\neural_de.jl:50\n",
      " [14] (::ODEFunction{false, SciMLBase.FullSpecialize, DiffEqFlux.var\"#dudt#19\"{StatefulLuxLayer{true, Chain{@NamedTuple{layer_1::BatchNorm{true, true, Float32, typeof(identity), typeof(zeros32), typeof(ones32)}, layer_2::Dense{true, typeof(relu), typeof(glorot_uniform), typeof(zeros32)}, layer_3::Dense{true, typeof(relu), typeof(glorot_uniform), typeof(zeros32)}, layer_4::Dense{true, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}}}, Nothing, @NamedTuple{layer_1::@NamedTuple{running_mean::Vector{Float32}, running_var::Vector{Float32}, training::Val{true}}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, LinearAlgebra.UniformScaling{Bool}, Nothing, typeof(DiffEqFlux.basic_tgrad), Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED), Nothing, Nothing, Nothing, Nothing})(::Matrix{Float64}, ::Vararg{Any})\n",
      "    @ SciMLBase C:\\Users\\heyye\\.julia\\packages\\SciMLBase\\rR75x\\src\\scimlfunctions.jl:2297\n",
      " [15] initialize!(integrator::OrdinaryDiffEq.ODEIntegrator{Euler, false, Matrix{Float64}, Nothing, Float64, ComponentVector{Float32, Vector{Float32}, Tuple{Axis{(layer_1 = ViewAxis(1:4, Axis(scale = 1:2, bias = 3:4)), layer_2 = ViewAxis(5:67, Axis(weight = ViewAxis(1:42, ShapedAxis((21, 2))), bias = ViewAxis(43:63, ShapedAxis((21, 1))))), layer_3 = ViewAxis(68:1453, Axis(weight = ViewAxis(1:1323, ShapedAxis((63, 21))), bias = ViewAxis(1324:1386, ShapedAxis((63, 1))))), layer_4 = ViewAxis(1454:4717, Axis(weight = ViewAxis(1:3213, ShapedAxis((51, 63))), bias = ViewAxis(3214:3264, ShapedAxis((51, 1))))))}}}, Float64, Float64, Float64, Float64, Vector{Matrix{Float64}}, ODESolution{Float64, 3, Vector{Matrix{Float64}}, Nothing, Nothing, Vector{Float64}, Vector{Vector{Matrix{Float64}}}, ODEProblem{Matrix{Float64}, Tuple{Float64, Float64}, false, ComponentVector{Float32, Vector{Float32}, Tuple{Axis{(layer_1 = ViewAxis(1:4, Axis(scale = 1:2, bias = 3:4)), layer_2 = ViewAxis(5:67, Axis(weight = ViewAxis(1:42, ShapedAxis((21, 2))), bias = ViewAxis(43:63, ShapedAxis((21, 1))))), layer_3 = ViewAxis(68:1453, Axis(weight = ViewAxis(1:1323, ShapedAxis((63, 21))), bias = ViewAxis(1324:1386, ShapedAxis((63, 1))))), layer_4 = ViewAxis(1454:4717, Axis(weight = ViewAxis(1:3213, ShapedAxis((51, 63))), bias = ViewAxis(3214:3264, ShapedAxis((51, 1))))))}}}, ODEFunction{false, SciMLBase.FullSpecialize, DiffEqFlux.var\"#dudt#19\"{StatefulLuxLayer{true, Chain{@NamedTuple{layer_1::BatchNorm{true, true, Float32, typeof(identity), typeof(zeros32), typeof(ones32)}, layer_2::Dense{true, typeof(relu), typeof(glorot_uniform), typeof(zeros32)}, layer_3::Dense{true, typeof(relu), typeof(glorot_uniform), typeof(zeros32)}, layer_4::Dense{true, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}}}, Nothing, @NamedTuple{layer_1::@NamedTuple{running_mean::Vector{Float32}, running_var::Vector{Float32}, training::Val{true}}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, LinearAlgebra.UniformScaling{Bool}, Nothing, typeof(DiffEqFlux.basic_tgrad), Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED), Nothing, Nothing, Nothing, Nothing}, @Kwargs{}, SciMLBase.StandardODEProblem}, Euler, OrdinaryDiffEq.InterpolationData{ODEFunction{false, SciMLBase.FullSpecialize, DiffEqFlux.var\"#dudt#19\"{StatefulLuxLayer{true, Chain{@NamedTuple{layer_1::BatchNorm{true, true, Float32, typeof(identity), typeof(zeros32), typeof(ones32)}, layer_2::Dense{true, typeof(relu), typeof(glorot_uniform), typeof(zeros32)}, layer_3::Dense{true, typeof(relu), typeof(glorot_uniform), typeof(zeros32)}, layer_4::Dense{true, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}}}, Nothing, @NamedTuple{layer_1::@NamedTuple{running_mean::Vector{Float32}, running_var::Vector{Float32}, training::Val{true}}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, LinearAlgebra.UniformScaling{Bool}, Nothing, typeof(DiffEqFlux.basic_tgrad), Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED), Nothing, Nothing, Nothing, Nothing}, Vector{Matrix{Float64}}, Vector{Float64}, Vector{Vector{Matrix{Float64}}}, Nothing, OrdinaryDiffEq.EulerConstantCache, Nothing}, SciMLBase.DEStats, Nothing, Nothing, Nothing}, ODEFunction{false, SciMLBase.FullSpecialize, DiffEqFlux.var\"#dudt#19\"{StatefulLuxLayer{true, Chain{@NamedTuple{layer_1::BatchNorm{true, true, Float32, typeof(identity), typeof(zeros32), typeof(ones32)}, layer_2::Dense{true, typeof(relu), typeof(glorot_uniform), typeof(zeros32)}, layer_3::Dense{true, typeof(relu), typeof(glorot_uniform), typeof(zeros32)}, layer_4::Dense{true, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}}}, Nothing, @NamedTuple{layer_1::@NamedTuple{running_mean::Vector{Float32}, running_var::Vector{Float32}, training::Val{true}}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, LinearAlgebra.UniformScaling{Bool}, Nothing, typeof(DiffEqFlux.basic_tgrad), Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED), Nothing, Nothing, Nothing, Nothing}, OrdinaryDiffEq.EulerConstantCache, OrdinaryDiffEq.DEOptions{Float64, Float64, Float64, Float64, PIController{Int64}, typeof(DiffEqBase.ODE_DEFAULT_NORM), typeof(LinearAlgebra.opnorm), Nothing, CallbackSet{Tuple{}, Tuple{}}, typeof(DiffEqBase.ODE_DEFAULT_ISOUTOFDOMAIN), typeof(DiffEqBase.ODE_DEFAULT_PROG_MESSAGE), typeof(DiffEqBase.ODE_DEFAULT_UNSTABLE_CHECK), DataStructures.BinaryHeap{Float64, DataStructures.FasterForward}, DataStructures.BinaryHeap{Float64, DataStructures.FasterForward}, Nothing, Nothing, Int64, Tuple{}, Float64, Tuple{}}, Matrix{Float64}, Float64, Nothing, OrdinaryDiffEq.DefaultInit, Nothing}, cache::OrdinaryDiffEq.EulerConstantCache)\n",
      "    @ OrdinaryDiffEq C:\\Users\\heyye\\.julia\\packages\\OrdinaryDiffEq\\HQ92J\\src\\perform_step\\fixed_timestep_perform_step.jl:48\n",
      " [16] __init(prob::ODEProblem{Matrix{Float64}, Tuple{Float64, Float64}, false, ComponentVector{Float32, Vector{Float32}, Tuple{Axis{(layer_1 = ViewAxis(1:4, Axis(scale = 1:2, bias = 3:4)), layer_2 = ViewAxis(5:67, Axis(weight = ViewAxis(1:42, ShapedAxis((21, 2))), bias = ViewAxis(43:63, ShapedAxis((21, 1))))), layer_3 = ViewAxis(68:1453, Axis(weight = ViewAxis(1:1323, ShapedAxis((63, 21))), bias = ViewAxis(1324:1386, ShapedAxis((63, 1))))), layer_4 = ViewAxis(1454:4717, Axis(weight = ViewAxis(1:3213, ShapedAxis((51, 63))), bias = ViewAxis(3214:3264, ShapedAxis((51, 1))))))}}}, ODEFunction{false, SciMLBase.FullSpecialize, DiffEqFlux.var\"#dudt#19\"{StatefulLuxLayer{true, Chain{@NamedTuple{layer_1::BatchNorm{true, true, Float32, typeof(identity), typeof(zeros32), typeof(ones32)}, layer_2::Dense{true, typeof(relu), typeof(glorot_uniform), typeof(zeros32)}, layer_3::Dense{true, typeof(relu), typeof(glorot_uniform), typeof(zeros32)}, layer_4::Dense{true, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}}}, Nothing, @NamedTuple{layer_1::@NamedTuple{running_mean::Vector{Float32}, running_var::Vector{Float32}, training::Val{true}}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, LinearAlgebra.UniformScaling{Bool}, Nothing, typeof(DiffEqFlux.basic_tgrad), Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED), Nothing, Nothing, Nothing, Nothing}, @Kwargs{}, SciMLBase.StandardODEProblem}, alg::Euler, timeseries_init::Tuple{}, ts_init::Tuple{}, ks_init::Tuple{}, recompile::Type{Val{true}}; saveat::Float64, tstops::Tuple{}, d_discontinuities::Tuple{}, save_idxs::Nothing, save_everystep::Bool, save_on::Bool, save_start::Bool, save_end::Nothing, callback::Nothing, dense::Bool, calck::Bool, dt::Float64, dtmin::Float64, dtmax::Float64, force_dtmin::Bool, adaptive::Bool, gamma::Int64, abstol::Nothing, reltol::Nothing, qmin::Int64, qmax::Int64, qsteady_min::Int64, qsteady_max::Int64, beta1::Nothing, beta2::Nothing, qoldinit::Int64, controller::Nothing, fullnormalize::Bool, failfactor::Int64, maxiters::Int64, internalnorm::typeof(DiffEqBase.ODE_DEFAULT_NORM), internalopnorm::typeof(LinearAlgebra.opnorm), isoutofdomain::typeof(DiffEqBase.ODE_DEFAULT_ISOUTOFDOMAIN), unstable_check::typeof(DiffEqBase.ODE_DEFAULT_UNSTABLE_CHECK), verbose::Bool, timeseries_errors::Bool, dense_errors::Bool, advance_to_tstop::Bool, stop_at_next_tstop::Bool, initialize_save::Bool, progress::Bool, progress_steps::Int64, progress_name::String, progress_message::typeof(DiffEqBase.ODE_DEFAULT_PROG_MESSAGE), progress_id::Symbol, userdata::Nothing, allow_extrapolation::Bool, initialize_integrator::Bool, alias_u0::Bool, alias_du0::Bool, initializealg::OrdinaryDiffEq.DefaultInit, kwargs::@Kwargs{})\n",
      "    @ OrdinaryDiffEq C:\\Users\\heyye\\.julia\\packages\\OrdinaryDiffEq\\HQ92J\\src\\solve.jl:524\n",
      " [17] __init (repeats 5 times)\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\OrdinaryDiffEq\\HQ92J\\src\\solve.jl:11 [inlined]\n",
      " [18] #__solve#560\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\OrdinaryDiffEq\\HQ92J\\src\\solve.jl:6 [inlined]\n",
      " [19] __solve\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\OrdinaryDiffEq\\HQ92J\\src\\solve.jl:1 [inlined]\n",
      " [20] solve_call(_prob::ODEProblem{Matrix{Float64}, Tuple{Float64, Float64}, false, ComponentVector{Float32, Vector{Float32}, Tuple{Axis{(layer_1 = ViewAxis(1:4, Axis(scale = 1:2, bias = 3:4)), layer_2 = ViewAxis(5:67, Axis(weight = ViewAxis(1:42, ShapedAxis((21, 2))), bias = ViewAxis(43:63, ShapedAxis((21, 1))))), layer_3 = ViewAxis(68:1453, Axis(weight = ViewAxis(1:1323, ShapedAxis((63, 21))), bias = ViewAxis(1324:1386, ShapedAxis((63, 1))))), layer_4 = ViewAxis(1454:4717, Axis(weight = ViewAxis(1:3213, ShapedAxis((51, 63))), bias = ViewAxis(3214:3264, ShapedAxis((51, 1))))))}}}, ODEFunction{false, SciMLBase.FullSpecialize, DiffEqFlux.var\"#dudt#19\"{StatefulLuxLayer{true, Chain{@NamedTuple{layer_1::BatchNorm{true, true, Float32, typeof(identity), typeof(zeros32), typeof(ones32)}, layer_2::Dense{true, typeof(relu), typeof(glorot_uniform), typeof(zeros32)}, layer_3::Dense{true, typeof(relu), typeof(glorot_uniform), typeof(zeros32)}, layer_4::Dense{true, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}}}, Nothing, @NamedTuple{layer_1::@NamedTuple{running_mean::Vector{Float32}, running_var::Vector{Float32}, training::Val{true}}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}}}}, LinearAlgebra.UniformScaling{Bool}, Nothing, typeof(DiffEqFlux.basic_tgrad), Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED), Nothing, Nothing, Nothing, Nothing}, @Kwargs{}, SciMLBase.StandardODEProblem}, args::Euler; merge_callbacks::Bool, kwargshandle::Nothing, kwargs::@Kwargs{saveat::Float64, dt::Float64})\n",
      "    @ DiffEqBase C:\\Users\\heyye\\.julia\\packages\\DiffEqBase\\c8MAQ\\src\\solve.jl:612\n",
      " [21] solve_call\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\DiffEqBase\\c8MAQ\\src\\solve.jl:569 [inlined]\n",
      " [22] #solve_up#53\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\DiffEqBase\\c8MAQ\\src\\solve.jl:1080 [inlined]\n",
      " [23] solve_up\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\DiffEqBase\\c8MAQ\\src\\solve.jl:1066 [inlined]\n",
      " [24] #solve#51\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\DiffEqBase\\c8MAQ\\src\\solve.jl:1003 [inlined]\n",
      " [25] solve\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\DiffEqBase\\c8MAQ\\src\\solve.jl:993 [inlined]\n",
      " [26] (::NeuralODE{Chain{@NamedTuple{layer_1::BatchNorm{true, true, Float32, typeof(identity), typeof(zeros32), typeof(ones32)}, layer_2::Dense{true, typeof(relu), typeof(glorot_uniform), typeof(zeros32)}, layer_3::Dense{true, typeof(relu), typeof(glorot_uniform), typeof(zeros32)}, layer_4::Dense{true, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}}}, Tuple{Float64, Float64}, Tuple{Euler}, @Kwargs{saveat::Float64, dt::Float64}})(x::Matrix{Float64}, p::ComponentVector{Float32, Vector{Float32}, Tuple{Axis{(layer_1 = ViewAxis(1:4, Axis(scale = 1:2, bias = 3:4)), layer_2 = ViewAxis(5:67, Axis(weight = ViewAxis(1:42, ShapedAxis((21, 2))), bias = ViewAxis(43:63, ShapedAxis((21, 1))))), layer_3 = ViewAxis(68:1453, Axis(weight = ViewAxis(1:1323, ShapedAxis((63, 21))), bias = ViewAxis(1324:1386, ShapedAxis((63, 1))))), layer_4 = ViewAxis(1454:4717, Axis(weight = ViewAxis(1:3213, ShapedAxis((51, 63))), bias = ViewAxis(3214:3264, ShapedAxis((51, 1))))))}}}, st::@NamedTuple{layer_1::@NamedTuple{running_mean::Vector{Float32}, running_var::Vector{Float32}, training::Val{true}}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}, layer_4::@NamedTuple{}})\n",
      "    @ DiffEqFlux C:\\Users\\heyye\\.julia\\packages\\DiffEqFlux\\TglmB\\src\\neural_de.jl:54\n",
      " [27] predict_neuralode(p::ComponentVector{Float32, Vector{Float32}, Tuple{Axis{(layer_1 = ViewAxis(1:4, Axis(scale = 1:2, bias = 3:4)), layer_2 = ViewAxis(5:67, Axis(weight = ViewAxis(1:42, ShapedAxis((21, 2))), bias = ViewAxis(43:63, ShapedAxis((21, 1))))), layer_3 = ViewAxis(68:1453, Axis(weight = ViewAxis(1:1323, ShapedAxis((63, 21))), bias = ViewAxis(1324:1386, ShapedAxis((63, 1))))), layer_4 = ViewAxis(1454:4717, Axis(weight = ViewAxis(1:3213, ShapedAxis((51, 63))), bias = ViewAxis(3214:3264, ShapedAxis((51, 1))))))}}})\n",
      "    @ Main g:\\My Drive\\Ga2O3\\DiffEqFlux\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_W5sZmlsZQ==.jl:2\n",
      " [28] loss_neuralode(p::ComponentVector{Float32, Vector{Float32}, Tuple{Axis{(layer_1 = ViewAxis(1:4, Axis(scale = 1:2, bias = 3:4)), layer_2 = ViewAxis(5:67, Axis(weight = ViewAxis(1:42, ShapedAxis((21, 2))), bias = ViewAxis(43:63, ShapedAxis((21, 1))))), layer_3 = ViewAxis(68:1453, Axis(weight = ViewAxis(1:1323, ShapedAxis((63, 21))), bias = ViewAxis(1324:1386, ShapedAxis((63, 1))))), layer_4 = ViewAxis(1454:4717, Axis(weight = ViewAxis(1:3213, ShapedAxis((51, 63))), bias = ViewAxis(3214:3264, ShapedAxis((51, 1))))))}}})\n",
      "    @ Main g:\\My Drive\\Ga2O3\\DiffEqFlux\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_W5sZmlsZQ==.jl:6\n",
      " [29] top-level scope\n",
      "    @ g:\\My Drive\\Ga2O3\\DiffEqFlux\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_W6sZmlsZQ==.jl:14"
     ]
    }
   ],
   "source": [
    "# Callback function to observe training\n",
    "callback = function (p, l, pred; doplot = false)\n",
    "    println(l)\n",
    "    # plot current prediction against data\n",
    "    if doplot\n",
    "        plt = scatter(tsteps, ode_data[1, :]; label = \"data\")\n",
    "        scatter!(plt, tsteps, pred[1, :]; label = \"prediction\")\n",
    "        display(plot(plt))\n",
    "    end\n",
    "    return false\n",
    "end\n",
    "\n",
    "pinit = ComponentArray(p)\n",
    "callback(pinit, loss_neuralode(pinit)...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "DimensionMismatch",
     "evalue": "DimensionMismatch: matrix A has dimensions (64,3), matrix B has dimensions (21,3)",
     "output_type": "error",
     "traceback": [
      "DimensionMismatch: matrix A has dimensions (64,3), matrix B has dimensions (21,3)\n",
      "\n",
      "Stacktrace:\n",
      "  [1] _generic_matmatmul!(C::Matrix{Float64}, tA::Char, tB::Char, A::Base.ReshapedArray{Float32, 2, SubArray{Float32, 1, Vector{Float32}, Tuple{UnitRange{Int64}}, true}, Tuple{}}, B::Matrix{Float64}, _add::LinearAlgebra.MulAddMul{true, true, Bool, Bool})\n",
      "    @ LinearAlgebra C:\\Users\\heyye\\.julia\\juliaup\\julia-1.10.1+0.x64.w64.mingw32\\share\\julia\\stdlib\\v1.10\\LinearAlgebra\\src\\matmul.jl:794\n",
      "  [2] generic_matmatmul!(C::Matrix{Float64}, tA::Char, tB::Char, A::Base.ReshapedArray{Float32, 2, SubArray{Float32, 1, Vector{Float32}, Tuple{UnitRange{Int64}}, true}, Tuple{}}, B::Matrix{Float64}, _add::LinearAlgebra.MulAddMul{true, true, Bool, Bool})\n",
      "    @ LinearAlgebra C:\\Users\\heyye\\.julia\\juliaup\\julia-1.10.1+0.x64.w64.mingw32\\share\\julia\\stdlib\\v1.10\\LinearAlgebra\\src\\matmul.jl:783\n",
      "  [3] mul!\n",
      "    @ C:\\Users\\heyye\\.julia\\juliaup\\julia-1.10.1+0.x64.w64.mingw32\\share\\julia\\stdlib\\v1.10\\LinearAlgebra\\src\\matmul.jl:263 [inlined]\n",
      "  [4] mul!\n",
      "    @ C:\\Users\\heyye\\.julia\\juliaup\\julia-1.10.1+0.x64.w64.mingw32\\share\\julia\\stdlib\\v1.10\\LinearAlgebra\\src\\matmul.jl:237 [inlined]\n",
      "  [5] __matmul!\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\LuxLib\\mwXSu\\src\\impl\\fused_dense.jl:5 [inlined]\n",
      "  [6] macro expansion\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\LuxLib\\mwXSu\\src\\impl\\fused_dense.jl:30 [inlined]\n",
      "  [7] __fused_dense_bias_activation_impl(act::typeof(tanh_fast), weight::Base.ReshapedArray{Float32, 2, SubArray{Float32, 1, Vector{Float32}, Tuple{UnitRange{Int64}}, true}, Tuple{}}, x::Matrix{Float64}, b::SubArray{Float32, 1, Vector{Float32}, Tuple{UnitRange{Int64}}, true})\n",
      "    @ LuxLib C:\\Users\\heyye\\.julia\\packages\\DispatchDoctor\\eWFc7\\src\\stabilization.jl:306\n",
      "  [8] fused_dense_bias_activation\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\LuxLib\\mwXSu\\src\\api\\dense.jl:46 [inlined]\n",
      "  [9] fused_dense_bias_activation\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\LuxLib\\mwXSu\\src\\api\\dense.jl:38 [inlined]\n",
      " [10] Dense\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\Lux\\LhwgF\\src\\layers\\basic.jl:357 [inlined]\n",
      " [11] apply\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\LuxCore\\9qfdM\\src\\LuxCore.jl:168 [inlined]\n",
      " [12] macro expansion\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\Lux\\LhwgF\\src\\layers\\containers.jl:0 [inlined]\n",
      " [13] applychain\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\Lux\\LhwgF\\src\\layers\\containers.jl:498 [inlined]\n",
      " [14] Chain\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\Lux\\LhwgF\\src\\layers\\containers.jl:496 [inlined]\n",
      " [15] apply\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\LuxCore\\9qfdM\\src\\LuxCore.jl:168 [inlined]\n",
      " [16] StatefulLuxLayer\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\Lux\\LhwgF\\src\\helpers\\stateful.jl:112 [inlined]\n",
      " [17] dudt\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\DiffEqFlux\\TglmB\\src\\neural_de.jl:50 [inlined]\n",
      " [18] ODEFunction\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\SciMLBase\\rR75x\\src\\scimlfunctions.jl:2297 [inlined]\n",
      " [19] initialize!(integrator::OrdinaryDiffEq.ODEIntegrator{Euler, false, Matrix{Float64}, Nothing, Float64, ComponentVector{Float32, Vector{Float32}, Tuple{Axis{(layer_1 = ViewAxis(1:256, Axis(weight = ViewAxis(1:192, ShapedAxis((64, 3))), bias = ViewAxis(193:256, ShapedAxis((64, 1))))), layer_2 = ViewAxis(257:8576, Axis(weight = ViewAxis(1:8192, ShapedAxis((128, 64))), bias = ViewAxis(8193:8320, ShapedAxis((128, 1))))), layer_3 = ViewAxis(8577:12704, Axis(weight = ViewAxis(1:4096, ShapedAxis((32, 128))), bias = ViewAxis(4097:4128, ShapedAxis((32, 1))))))}}}, Float64, Float64, Float64, Float64, Vector{Matrix{Float64}}, ODESolution{Float64, 3, Vector{Matrix{Float64}}, Nothing, Nothing, Vector{Float64}, Vector{Vector{Matrix{Float64}}}, ODEProblem{Matrix{Float64}, Tuple{Float64, Float64}, false, ComponentVector{Float32, Vector{Float32}, Tuple{Axis{(layer_1 = ViewAxis(1:256, Axis(weight = ViewAxis(1:192, ShapedAxis((64, 3))), bias = ViewAxis(193:256, ShapedAxis((64, 1))))), layer_2 = ViewAxis(257:8576, Axis(weight = ViewAxis(1:8192, ShapedAxis((128, 64))), bias = ViewAxis(8193:8320, ShapedAxis((128, 1))))), layer_3 = ViewAxis(8577:12704, Axis(weight = ViewAxis(1:4096, ShapedAxis((32, 128))), bias = ViewAxis(4097:4128, ShapedAxis((32, 1))))))}}}, ODEFunction{false, SciMLBase.FullSpecialize, DiffEqFlux.var\"#dudt#19\"{StatefulLuxLayer{true, Chain{@NamedTuple{layer_1::Dense{true, typeof(tanh_fast), typeof(glorot_uniform), typeof(zeros32)}, layer_2::Dense{true, typeof(relu), typeof(glorot_uniform), typeof(zeros32)}, layer_3::Dense{true, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}}}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}}}}, LinearAlgebra.UniformScaling{Bool}, Nothing, typeof(DiffEqFlux.basic_tgrad), Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED), Nothing, Nothing, Nothing, Nothing}, @Kwargs{}, SciMLBase.StandardODEProblem}, Euler, OrdinaryDiffEq.InterpolationData{ODEFunction{false, SciMLBase.FullSpecialize, DiffEqFlux.var\"#dudt#19\"{StatefulLuxLayer{true, Chain{@NamedTuple{layer_1::Dense{true, typeof(tanh_fast), typeof(glorot_uniform), typeof(zeros32)}, layer_2::Dense{true, typeof(relu), typeof(glorot_uniform), typeof(zeros32)}, layer_3::Dense{true, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}}}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}}}}, LinearAlgebra.UniformScaling{Bool}, Nothing, typeof(DiffEqFlux.basic_tgrad), Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED), Nothing, Nothing, Nothing, Nothing}, Vector{Matrix{Float64}}, Vector{Float64}, Vector{Vector{Matrix{Float64}}}, Nothing, OrdinaryDiffEq.EulerConstantCache, Nothing}, SciMLBase.DEStats, Nothing, Nothing, Nothing}, ODEFunction{false, SciMLBase.FullSpecialize, DiffEqFlux.var\"#dudt#19\"{StatefulLuxLayer{true, Chain{@NamedTuple{layer_1::Dense{true, typeof(tanh_fast), typeof(glorot_uniform), typeof(zeros32)}, layer_2::Dense{true, typeof(relu), typeof(glorot_uniform), typeof(zeros32)}, layer_3::Dense{true, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}}}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}}}}, LinearAlgebra.UniformScaling{Bool}, Nothing, typeof(DiffEqFlux.basic_tgrad), Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED), Nothing, Nothing, Nothing, Nothing}, OrdinaryDiffEq.EulerConstantCache, OrdinaryDiffEq.DEOptions{Float64, Float64, Float64, Float64, PIController{Int64}, typeof(DiffEqBase.ODE_DEFAULT_NORM), typeof(LinearAlgebra.opnorm), Bool, CallbackSet{Tuple{}, Tuple{}}, typeof(DiffEqBase.ODE_DEFAULT_ISOUTOFDOMAIN), typeof(DiffEqBase.ODE_DEFAULT_PROG_MESSAGE), typeof(DiffEqBase.ODE_DEFAULT_UNSTABLE_CHECK), DataStructures.BinaryHeap{Float64, DataStructures.FasterForward}, DataStructures.BinaryHeap{Float64, DataStructures.FasterForward}, Nothing, Nothing, Int64, Tuple{}, Tuple{}, Tuple{}}, Matrix{Float64}, Float64, Nothing, OrdinaryDiffEq.DefaultInit, Nothing}, cache::OrdinaryDiffEq.EulerConstantCache)\n",
      "    @ OrdinaryDiffEq C:\\Users\\heyye\\.julia\\packages\\OrdinaryDiffEq\\HQ92J\\src\\perform_step\\fixed_timestep_perform_step.jl:48\n",
      " [20] __init(prob::ODEProblem{Matrix{Float64}, Tuple{Float64, Float64}, false, ComponentVector{Float32, Vector{Float32}, Tuple{Axis{(layer_1 = ViewAxis(1:256, Axis(weight = ViewAxis(1:192, ShapedAxis((64, 3))), bias = ViewAxis(193:256, ShapedAxis((64, 1))))), layer_2 = ViewAxis(257:8576, Axis(weight = ViewAxis(1:8192, ShapedAxis((128, 64))), bias = ViewAxis(8193:8320, ShapedAxis((128, 1))))), layer_3 = ViewAxis(8577:12704, Axis(weight = ViewAxis(1:4096, ShapedAxis((32, 128))), bias = ViewAxis(4097:4128, ShapedAxis((32, 1))))))}}}, ODEFunction{false, SciMLBase.FullSpecialize, DiffEqFlux.var\"#dudt#19\"{StatefulLuxLayer{true, Chain{@NamedTuple{layer_1::Dense{true, typeof(tanh_fast), typeof(glorot_uniform), typeof(zeros32)}, layer_2::Dense{true, typeof(relu), typeof(glorot_uniform), typeof(zeros32)}, layer_3::Dense{true, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}}}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}}}}, LinearAlgebra.UniformScaling{Bool}, Nothing, typeof(DiffEqFlux.basic_tgrad), Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED), Nothing, Nothing, Nothing, Nothing}, @Kwargs{}, SciMLBase.StandardODEProblem}, alg::Euler, timeseries_init::Tuple{}, ts_init::Tuple{}, ks_init::Tuple{}, recompile::Type{Val{true}}; saveat::Tuple{}, tstops::Tuple{}, d_discontinuities::Tuple{}, save_idxs::Nothing, save_everystep::Bool, save_on::Bool, save_start::Bool, save_end::Bool, callback::Nothing, dense::Bool, calck::Bool, dt::Float64, dtmin::Float64, dtmax::Float64, force_dtmin::Bool, adaptive::Bool, gamma::Int64, abstol::Nothing, reltol::Nothing, qmin::Int64, qmax::Int64, qsteady_min::Int64, qsteady_max::Int64, beta1::Nothing, beta2::Nothing, qoldinit::Int64, controller::Nothing, fullnormalize::Bool, failfactor::Int64, maxiters::Int64, internalnorm::typeof(DiffEqBase.ODE_DEFAULT_NORM), internalopnorm::typeof(LinearAlgebra.opnorm), isoutofdomain::typeof(DiffEqBase.ODE_DEFAULT_ISOUTOFDOMAIN), unstable_check::typeof(DiffEqBase.ODE_DEFAULT_UNSTABLE_CHECK), verbose::Bool, timeseries_errors::Bool, dense_errors::Bool, advance_to_tstop::Bool, stop_at_next_tstop::Bool, initialize_save::Bool, progress::Bool, progress_steps::Int64, progress_name::String, progress_message::typeof(DiffEqBase.ODE_DEFAULT_PROG_MESSAGE), progress_id::Symbol, userdata::Nothing, allow_extrapolation::Bool, initialize_integrator::Bool, alias_u0::Bool, alias_du0::Bool, initializealg::OrdinaryDiffEq.DefaultInit, kwargs::@Kwargs{save_noise::Bool})\n",
      "    @ OrdinaryDiffEq C:\\Users\\heyye\\.julia\\packages\\OrdinaryDiffEq\\HQ92J\\src\\solve.jl:524\n",
      " [21] __init (repeats 5 times)\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\OrdinaryDiffEq\\HQ92J\\src\\solve.jl:11 [inlined]\n",
      " [22] #__solve#560\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\OrdinaryDiffEq\\HQ92J\\src\\solve.jl:6 [inlined]\n",
      " [23] __solve\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\OrdinaryDiffEq\\HQ92J\\src\\solve.jl:1 [inlined]\n",
      " [24] solve_call(_prob::ODEProblem{Matrix{Float64}, Tuple{Float64, Float64}, false, ComponentVector{Float32, Vector{Float32}, Tuple{Axis{(layer_1 = ViewAxis(1:256, Axis(weight = ViewAxis(1:192, ShapedAxis((64, 3))), bias = ViewAxis(193:256, ShapedAxis((64, 1))))), layer_2 = ViewAxis(257:8576, Axis(weight = ViewAxis(1:8192, ShapedAxis((128, 64))), bias = ViewAxis(8193:8320, ShapedAxis((128, 1))))), layer_3 = ViewAxis(8577:12704, Axis(weight = ViewAxis(1:4096, ShapedAxis((32, 128))), bias = ViewAxis(4097:4128, ShapedAxis((32, 1))))))}}}, ODEFunction{false, SciMLBase.FullSpecialize, DiffEqFlux.var\"#dudt#19\"{StatefulLuxLayer{true, Chain{@NamedTuple{layer_1::Dense{true, typeof(tanh_fast), typeof(glorot_uniform), typeof(zeros32)}, layer_2::Dense{true, typeof(relu), typeof(glorot_uniform), typeof(zeros32)}, layer_3::Dense{true, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}}}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}}}}, LinearAlgebra.UniformScaling{Bool}, Nothing, typeof(DiffEqFlux.basic_tgrad), Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED), Nothing, Nothing, Nothing, Nothing}, @Kwargs{}, SciMLBase.StandardODEProblem}, args::Euler; merge_callbacks::Bool, kwargshandle::Nothing, kwargs::@Kwargs{save_noise::Bool, save_start::Bool, save_end::Bool, dt::Float64})\n",
      "    @ DiffEqBase C:\\Users\\heyye\\.julia\\packages\\DiffEqBase\\c8MAQ\\src\\solve.jl:612\n",
      " [25] solve_call\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\DiffEqBase\\c8MAQ\\src\\solve.jl:569 [inlined]\n",
      " [26] #solve_up#53\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\DiffEqBase\\c8MAQ\\src\\solve.jl:1080 [inlined]\n",
      " [27] solve_up\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\DiffEqBase\\c8MAQ\\src\\solve.jl:1066 [inlined]\n",
      " [28] #solve#51\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\DiffEqBase\\c8MAQ\\src\\solve.jl:1003 [inlined]\n",
      " [29] _concrete_solve_adjoint(::ODEProblem{Matrix{Float64}, Tuple{Float64, Float64}, false, ComponentVector{Float32, Vector{Float32}, Tuple{Axis{(layer_1 = ViewAxis(1:256, Axis(weight = ViewAxis(1:192, ShapedAxis((64, 3))), bias = ViewAxis(193:256, ShapedAxis((64, 1))))), layer_2 = ViewAxis(257:8576, Axis(weight = ViewAxis(1:8192, ShapedAxis((128, 64))), bias = ViewAxis(8193:8320, ShapedAxis((128, 1))))), layer_3 = ViewAxis(8577:12704, Axis(weight = ViewAxis(1:4096, ShapedAxis((32, 128))), bias = ViewAxis(4097:4128, ShapedAxis((32, 1))))))}}}, ODEFunction{false, SciMLBase.FullSpecialize, DiffEqFlux.var\"#dudt#19\"{StatefulLuxLayer{true, Chain{@NamedTuple{layer_1::Dense{true, typeof(tanh_fast), typeof(glorot_uniform), typeof(zeros32)}, layer_2::Dense{true, typeof(relu), typeof(glorot_uniform), typeof(zeros32)}, layer_3::Dense{true, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}}}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}}}}, LinearAlgebra.UniformScaling{Bool}, Nothing, typeof(DiffEqFlux.basic_tgrad), Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED), Nothing, Nothing, Nothing, Nothing}, @Kwargs{}, SciMLBase.StandardODEProblem}, ::Euler, ::InterpolatingAdjoint{0, true, Val{:central}, ZygoteVJP}, ::Matrix{Float64}, ::ComponentVector{Float32, Vector{Float32}, Tuple{Axis{(layer_1 = ViewAxis(1:256, Axis(weight = ViewAxis(1:192, ShapedAxis((64, 3))), bias = ViewAxis(193:256, ShapedAxis((64, 1))))), layer_2 = ViewAxis(257:8576, Axis(weight = ViewAxis(1:8192, ShapedAxis((128, 64))), bias = ViewAxis(8193:8320, ShapedAxis((128, 1))))), layer_3 = ViewAxis(8577:12704, Axis(weight = ViewAxis(1:4096, ShapedAxis((32, 128))), bias = ViewAxis(4097:4128, ShapedAxis((32, 1))))))}}}, ::SciMLBase.ChainRulesOriginator; save_start::Bool, save_end::Bool, saveat::Float64, save_idxs::Nothing, kwargs::@Kwargs{dt::Float64})\n",
      "    @ SciMLSensitivity C:\\Users\\heyye\\.julia\\packages\\SciMLSensitivity\\4YtYh\\src\\concrete_solve.jl:422\n",
      " [30] _concrete_solve_adjoint\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\SciMLSensitivity\\4YtYh\\src\\concrete_solve.jl:359 [inlined]\n",
      " [31] #_solve_adjoint#75\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\DiffEqBase\\c8MAQ\\src\\solve.jl:1537 [inlined]\n",
      " [32] _solve_adjoint\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\DiffEqBase\\c8MAQ\\src\\solve.jl:1510 [inlined]\n",
      " [33] #rrule#4\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\DiffEqBase\\c8MAQ\\ext\\DiffEqBaseChainRulesCoreExt.jl:26 [inlined]\n",
      " [34] rrule\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\DiffEqBase\\c8MAQ\\ext\\DiffEqBaseChainRulesCoreExt.jl:22 [inlined]\n",
      " [35] rrule\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\ChainRulesCore\\I1EbV\\src\\rules.jl:140 [inlined]\n",
      " [36] chain_rrule_kw\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\Zygote\\nsBv0\\src\\compiler\\chainrules.jl:235 [inlined]\n",
      " [37] macro expansion\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\Zygote\\nsBv0\\src\\compiler\\interface2.jl:0 [inlined]\n",
      " [38] _pullback(::Zygote.Context{false}, ::typeof(Core.kwcall), ::@NamedTuple{saveat::Float64, dt::Float64}, ::typeof(DiffEqBase.solve_up), ::ODEProblem{Matrix{Float64}, Tuple{Float64, Float64}, false, ComponentVector{Float32, Vector{Float32}, Tuple{Axis{(layer_1 = ViewAxis(1:256, Axis(weight = ViewAxis(1:192, ShapedAxis((64, 3))), bias = ViewAxis(193:256, ShapedAxis((64, 1))))), layer_2 = ViewAxis(257:8576, Axis(weight = ViewAxis(1:8192, ShapedAxis((128, 64))), bias = ViewAxis(8193:8320, ShapedAxis((128, 1))))), layer_3 = ViewAxis(8577:12704, Axis(weight = ViewAxis(1:4096, ShapedAxis((32, 128))), bias = ViewAxis(4097:4128, ShapedAxis((32, 1))))))}}}, ODEFunction{false, SciMLBase.FullSpecialize, DiffEqFlux.var\"#dudt#19\"{StatefulLuxLayer{true, Chain{@NamedTuple{layer_1::Dense{true, typeof(tanh_fast), typeof(glorot_uniform), typeof(zeros32)}, layer_2::Dense{true, typeof(relu), typeof(glorot_uniform), typeof(zeros32)}, layer_3::Dense{true, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}}}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}}}}, LinearAlgebra.UniformScaling{Bool}, Nothing, typeof(DiffEqFlux.basic_tgrad), Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED), Nothing, Nothing, Nothing, Nothing}, @Kwargs{}, SciMLBase.StandardODEProblem}, ::InterpolatingAdjoint{0, true, Val{:central}, ZygoteVJP}, ::Matrix{Float64}, ::ComponentVector{Float32, Vector{Float32}, Tuple{Axis{(layer_1 = ViewAxis(1:256, Axis(weight = ViewAxis(1:192, ShapedAxis((64, 3))), bias = ViewAxis(193:256, ShapedAxis((64, 1))))), layer_2 = ViewAxis(257:8576, Axis(weight = ViewAxis(1:8192, ShapedAxis((128, 64))), bias = ViewAxis(8193:8320, ShapedAxis((128, 1))))), layer_3 = ViewAxis(8577:12704, Axis(weight = ViewAxis(1:4096, ShapedAxis((32, 128))), bias = ViewAxis(4097:4128, ShapedAxis((32, 1))))))}}}, ::Euler)\n",
      "    @ Zygote C:\\Users\\heyye\\.julia\\packages\\Zygote\\nsBv0\\src\\compiler\\interface2.jl:87\n",
      " [39] _apply(::Function, ::Vararg{Any})\n",
      "    @ Core .\\boot.jl:838\n",
      " [40] adjoint\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\Zygote\\nsBv0\\src\\lib\\lib.jl:203 [inlined]\n",
      " [41] _pullback\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\ZygoteRules\\M4xmc\\src\\adjoint.jl:67 [inlined]\n",
      " [42] #solve#51\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\DiffEqBase\\c8MAQ\\src\\solve.jl:1003 [inlined]\n",
      " [43] _pullback(::Zygote.Context{false}, ::DiffEqBase.var\"##solve#51\", ::InterpolatingAdjoint{0, true, Val{:central}, ZygoteVJP}, ::Nothing, ::Nothing, ::Val{true}, ::@Kwargs{saveat::Float64, dt::Float64}, ::typeof(solve), ::ODEProblem{Matrix{Float64}, Tuple{Float64, Float64}, false, ComponentVector{Float32, Vector{Float32}, Tuple{Axis{(layer_1 = ViewAxis(1:256, Axis(weight = ViewAxis(1:192, ShapedAxis((64, 3))), bias = ViewAxis(193:256, ShapedAxis((64, 1))))), layer_2 = ViewAxis(257:8576, Axis(weight = ViewAxis(1:8192, ShapedAxis((128, 64))), bias = ViewAxis(8193:8320, ShapedAxis((128, 1))))), layer_3 = ViewAxis(8577:12704, Axis(weight = ViewAxis(1:4096, ShapedAxis((32, 128))), bias = ViewAxis(4097:4128, ShapedAxis((32, 1))))))}}}, ODEFunction{false, SciMLBase.FullSpecialize, DiffEqFlux.var\"#dudt#19\"{StatefulLuxLayer{true, Chain{@NamedTuple{layer_1::Dense{true, typeof(tanh_fast), typeof(glorot_uniform), typeof(zeros32)}, layer_2::Dense{true, typeof(relu), typeof(glorot_uniform), typeof(zeros32)}, layer_3::Dense{true, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}}}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}}}}, LinearAlgebra.UniformScaling{Bool}, Nothing, typeof(DiffEqFlux.basic_tgrad), Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED), Nothing, Nothing, Nothing, Nothing}, @Kwargs{}, SciMLBase.StandardODEProblem}, ::Euler)\n",
      "    @ Zygote C:\\Users\\heyye\\.julia\\packages\\Zygote\\nsBv0\\src\\compiler\\interface2.jl:0\n",
      " [44] _apply(::Function, ::Vararg{Any})\n",
      "    @ Core .\\boot.jl:838\n",
      " [45] adjoint\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\Zygote\\nsBv0\\src\\lib\\lib.jl:203 [inlined]\n",
      " [46] _pullback\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\ZygoteRules\\M4xmc\\src\\adjoint.jl:67 [inlined]\n",
      " [47] solve\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\DiffEqBase\\c8MAQ\\src\\solve.jl:993 [inlined]\n",
      " [48] _pullback(::Zygote.Context{false}, ::typeof(Core.kwcall), ::@NamedTuple{sensealg::InterpolatingAdjoint{0, true, Val{:central}, ZygoteVJP}, saveat::Float64, dt::Float64}, ::typeof(solve), ::ODEProblem{Matrix{Float64}, Tuple{Float64, Float64}, false, ComponentVector{Float32, Vector{Float32}, Tuple{Axis{(layer_1 = ViewAxis(1:256, Axis(weight = ViewAxis(1:192, ShapedAxis((64, 3))), bias = ViewAxis(193:256, ShapedAxis((64, 1))))), layer_2 = ViewAxis(257:8576, Axis(weight = ViewAxis(1:8192, ShapedAxis((128, 64))), bias = ViewAxis(8193:8320, ShapedAxis((128, 1))))), layer_3 = ViewAxis(8577:12704, Axis(weight = ViewAxis(1:4096, ShapedAxis((32, 128))), bias = ViewAxis(4097:4128, ShapedAxis((32, 1))))))}}}, ODEFunction{false, SciMLBase.FullSpecialize, DiffEqFlux.var\"#dudt#19\"{StatefulLuxLayer{true, Chain{@NamedTuple{layer_1::Dense{true, typeof(tanh_fast), typeof(glorot_uniform), typeof(zeros32)}, layer_2::Dense{true, typeof(relu), typeof(glorot_uniform), typeof(zeros32)}, layer_3::Dense{true, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}}}, Nothing, @NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}}}}, LinearAlgebra.UniformScaling{Bool}, Nothing, typeof(DiffEqFlux.basic_tgrad), Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED), Nothing, Nothing, Nothing, Nothing}, @Kwargs{}, SciMLBase.StandardODEProblem}, ::Euler)\n",
      "    @ Zygote C:\\Users\\heyye\\.julia\\packages\\Zygote\\nsBv0\\src\\compiler\\interface2.jl:0\n",
      " [49] _apply(::Function, ::Vararg{Any})\n",
      "    @ Core .\\boot.jl:838\n",
      " [50] adjoint\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\Zygote\\nsBv0\\src\\lib\\lib.jl:203 [inlined]\n",
      " [51] _pullback\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\ZygoteRules\\M4xmc\\src\\adjoint.jl:67 [inlined]\n",
      " [52] NeuralODE\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\DiffEqFlux\\TglmB\\src\\neural_de.jl:54 [inlined]\n",
      " [53] _pullback(::Zygote.Context{false}, ::NeuralODE{Chain{@NamedTuple{layer_1::Dense{true, typeof(tanh_fast), typeof(glorot_uniform), typeof(zeros32)}, layer_2::Dense{true, typeof(relu), typeof(glorot_uniform), typeof(zeros32)}, layer_3::Dense{true, typeof(identity), typeof(glorot_uniform), typeof(zeros32)}}}, Tuple{Float64, Float64}, Tuple{Euler}, @Kwargs{saveat::Float64, dt::Float64}}, ::Matrix{Float64}, ::ComponentVector{Float32, Vector{Float32}, Tuple{Axis{(layer_1 = ViewAxis(1:256, Axis(weight = ViewAxis(1:192, ShapedAxis((64, 3))), bias = ViewAxis(193:256, ShapedAxis((64, 1))))), layer_2 = ViewAxis(257:8576, Axis(weight = ViewAxis(1:8192, ShapedAxis((128, 64))), bias = ViewAxis(8193:8320, ShapedAxis((128, 1))))), layer_3 = ViewAxis(8577:12704, Axis(weight = ViewAxis(1:4096, ShapedAxis((32, 128))), bias = ViewAxis(4097:4128, ShapedAxis((32, 1))))))}}}, ::@NamedTuple{layer_1::@NamedTuple{}, layer_2::@NamedTuple{}, layer_3::@NamedTuple{}})\n",
      "    @ Zygote C:\\Users\\heyye\\.julia\\packages\\Zygote\\nsBv0\\src\\compiler\\interface2.jl:0\n",
      " [54] predict_neuralode\n",
      "    @ g:\\My Drive\\Ga2O3\\DiffEqFlux\\AM-NODE.ipynb:2 [inlined]\n",
      " [55] _pullback(ctx::Zygote.Context{false}, f::typeof(predict_neuralode), args::ComponentVector{Float32, Vector{Float32}, Tuple{Axis{(layer_1 = ViewAxis(1:256, Axis(weight = ViewAxis(1:192, ShapedAxis((64, 3))), bias = ViewAxis(193:256, ShapedAxis((64, 1))))), layer_2 = ViewAxis(257:8576, Axis(weight = ViewAxis(1:8192, ShapedAxis((128, 64))), bias = ViewAxis(8193:8320, ShapedAxis((128, 1))))), layer_3 = ViewAxis(8577:12704, Axis(weight = ViewAxis(1:4096, ShapedAxis((32, 128))), bias = ViewAxis(4097:4128, ShapedAxis((32, 1))))))}}})\n",
      "    @ Zygote C:\\Users\\heyye\\.julia\\packages\\Zygote\\nsBv0\\src\\compiler\\interface2.jl:0\n",
      " [56] loss_neuralode\n",
      "    @ g:\\My Drive\\Ga2O3\\DiffEqFlux\\AM-NODE.ipynb:6 [inlined]\n",
      " [57] _pullback(ctx::Zygote.Context{false}, f::typeof(loss_neuralode), args::ComponentVector{Float32, Vector{Float32}, Tuple{Axis{(layer_1 = ViewAxis(1:256, Axis(weight = ViewAxis(1:192, ShapedAxis((64, 3))), bias = ViewAxis(193:256, ShapedAxis((64, 1))))), layer_2 = ViewAxis(257:8576, Axis(weight = ViewAxis(1:8192, ShapedAxis((128, 64))), bias = ViewAxis(8193:8320, ShapedAxis((128, 1))))), layer_3 = ViewAxis(8577:12704, Axis(weight = ViewAxis(1:4096, ShapedAxis((32, 128))), bias = ViewAxis(4097:4128, ShapedAxis((32, 1))))))}}})\n",
      "    @ Zygote C:\\Users\\heyye\\.julia\\packages\\Zygote\\nsBv0\\src\\compiler\\interface2.jl:0\n",
      " [58] #12\n",
      "    @ g:\\My Drive\\Ga2O3\\DiffEqFlux\\AM-NODE.ipynb:4 [inlined]\n",
      " [59] _pullback(::Zygote.Context{false}, ::var\"#12#13\", ::ComponentVector{Float32, Vector{Float32}, Tuple{Axis{(layer_1 = ViewAxis(1:256, Axis(weight = ViewAxis(1:192, ShapedAxis((64, 3))), bias = ViewAxis(193:256, ShapedAxis((64, 1))))), layer_2 = ViewAxis(257:8576, Axis(weight = ViewAxis(1:8192, ShapedAxis((128, 64))), bias = ViewAxis(8193:8320, ShapedAxis((128, 1))))), layer_3 = ViewAxis(8577:12704, Axis(weight = ViewAxis(1:4096, ShapedAxis((32, 128))), bias = ViewAxis(4097:4128, ShapedAxis((32, 1))))))}}}, ::SciMLBase.NullParameters)\n",
      "    @ Zygote C:\\Users\\heyye\\.julia\\packages\\Zygote\\nsBv0\\src\\compiler\\interface2.jl:0\n",
      " [60] _apply\n",
      "    @ .\\boot.jl:838 [inlined]\n",
      " [61] adjoint\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\Zygote\\nsBv0\\src\\lib\\lib.jl:203 [inlined]\n",
      " [62] _pullback\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\ZygoteRules\\M4xmc\\src\\adjoint.jl:67 [inlined]\n",
      " [63] OptimizationFunction\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\SciMLBase\\rR75x\\src\\scimlfunctions.jl:3763 [inlined]\n",
      " [64] _pullback(::Zygote.Context{false}, ::OptimizationFunction{true, AutoZygote, var\"#12#13\", Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED_NO_TIME), Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing}, ::ComponentVector{Float32, Vector{Float32}, Tuple{Axis{(layer_1 = ViewAxis(1:256, Axis(weight = ViewAxis(1:192, ShapedAxis((64, 3))), bias = ViewAxis(193:256, ShapedAxis((64, 1))))), layer_2 = ViewAxis(257:8576, Axis(weight = ViewAxis(1:8192, ShapedAxis((128, 64))), bias = ViewAxis(8193:8320, ShapedAxis((128, 1))))), layer_3 = ViewAxis(8577:12704, Axis(weight = ViewAxis(1:4096, ShapedAxis((32, 128))), bias = ViewAxis(4097:4128, ShapedAxis((32, 1))))))}}}, ::SciMLBase.NullParameters)\n",
      "    @ Zygote C:\\Users\\heyye\\.julia\\packages\\Zygote\\nsBv0\\src\\compiler\\interface2.jl:0\n",
      " [65] _apply(::Function, ::Vararg{Any})\n",
      "    @ Core .\\boot.jl:838\n",
      " [66] adjoint\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\Zygote\\nsBv0\\src\\lib\\lib.jl:203 [inlined]\n",
      " [67] _pullback\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\ZygoteRules\\M4xmc\\src\\adjoint.jl:67 [inlined]\n",
      " [68] #37\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\OptimizationBase\\32Mb0\\ext\\OptimizationZygoteExt.jl:90 [inlined]\n",
      " [69] _pullback(ctx::Zygote.Context{false}, f::OptimizationZygoteExt.var\"#37#55\"{OptimizationFunction{true, AutoZygote, var\"#12#13\", Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED_NO_TIME), Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing}, OptimizationBase.ReInitCache{ComponentVector{Float32, Vector{Float32}, Tuple{Axis{(layer_1 = ViewAxis(1:256, Axis(weight = ViewAxis(1:192, ShapedAxis((64, 3))), bias = ViewAxis(193:256, ShapedAxis((64, 1))))), layer_2 = ViewAxis(257:8576, Axis(weight = ViewAxis(1:8192, ShapedAxis((128, 64))), bias = ViewAxis(8193:8320, ShapedAxis((128, 1))))), layer_3 = ViewAxis(8577:12704, Axis(weight = ViewAxis(1:4096, ShapedAxis((32, 128))), bias = ViewAxis(4097:4128, ShapedAxis((32, 1))))))}}}, SciMLBase.NullParameters}}, args::ComponentVector{Float32, Vector{Float32}, Tuple{Axis{(layer_1 = ViewAxis(1:256, Axis(weight = ViewAxis(1:192, ShapedAxis((64, 3))), bias = ViewAxis(193:256, ShapedAxis((64, 1))))), layer_2 = ViewAxis(257:8576, Axis(weight = ViewAxis(1:8192, ShapedAxis((128, 64))), bias = ViewAxis(8193:8320, ShapedAxis((128, 1))))), layer_3 = ViewAxis(8577:12704, Axis(weight = ViewAxis(1:4096, ShapedAxis((32, 128))), bias = ViewAxis(4097:4128, ShapedAxis((32, 1))))))}}})\n",
      "    @ Zygote C:\\Users\\heyye\\.julia\\packages\\Zygote\\nsBv0\\src\\compiler\\interface2.jl:0\n",
      " [70] _apply(::Function, ::Vararg{Any})\n",
      "    @ Core .\\boot.jl:838\n",
      " [71] adjoint\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\Zygote\\nsBv0\\src\\lib\\lib.jl:203 [inlined]\n",
      " [72] _pullback\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\ZygoteRules\\M4xmc\\src\\adjoint.jl:67 [inlined]\n",
      " [73] #39\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\OptimizationBase\\32Mb0\\ext\\OptimizationZygoteExt.jl:93 [inlined]\n",
      " [74] _pullback(ctx::Zygote.Context{false}, f::OptimizationZygoteExt.var\"#39#57\"{Tuple{}, OptimizationZygoteExt.var\"#37#55\"{OptimizationFunction{true, AutoZygote, var\"#12#13\", Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED_NO_TIME), Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing}, OptimizationBase.ReInitCache{ComponentVector{Float32, Vector{Float32}, Tuple{Axis{(layer_1 = ViewAxis(1:256, Axis(weight = ViewAxis(1:192, ShapedAxis((64, 3))), bias = ViewAxis(193:256, ShapedAxis((64, 1))))), layer_2 = ViewAxis(257:8576, Axis(weight = ViewAxis(1:8192, ShapedAxis((128, 64))), bias = ViewAxis(8193:8320, ShapedAxis((128, 1))))), layer_3 = ViewAxis(8577:12704, Axis(weight = ViewAxis(1:4096, ShapedAxis((32, 128))), bias = ViewAxis(4097:4128, ShapedAxis((32, 1))))))}}}, SciMLBase.NullParameters}}}, args::ComponentVector{Float32, Vector{Float32}, Tuple{Axis{(layer_1 = ViewAxis(1:256, Axis(weight = ViewAxis(1:192, ShapedAxis((64, 3))), bias = ViewAxis(193:256, ShapedAxis((64, 1))))), layer_2 = ViewAxis(257:8576, Axis(weight = ViewAxis(1:8192, ShapedAxis((128, 64))), bias = ViewAxis(8193:8320, ShapedAxis((128, 1))))), layer_3 = ViewAxis(8577:12704, Axis(weight = ViewAxis(1:4096, ShapedAxis((32, 128))), bias = ViewAxis(4097:4128, ShapedAxis((32, 1))))))}}})\n",
      "    @ Zygote C:\\Users\\heyye\\.julia\\packages\\Zygote\\nsBv0\\src\\compiler\\interface2.jl:0\n",
      " [75] pullback(f::Function, cx::Zygote.Context{false}, args::ComponentVector{Float32, Vector{Float32}, Tuple{Axis{(layer_1 = ViewAxis(1:256, Axis(weight = ViewAxis(1:192, ShapedAxis((64, 3))), bias = ViewAxis(193:256, ShapedAxis((64, 1))))), layer_2 = ViewAxis(257:8576, Axis(weight = ViewAxis(1:8192, ShapedAxis((128, 64))), bias = ViewAxis(8193:8320, ShapedAxis((128, 1))))), layer_3 = ViewAxis(8577:12704, Axis(weight = ViewAxis(1:4096, ShapedAxis((32, 128))), bias = ViewAxis(4097:4128, ShapedAxis((32, 1))))))}}})\n",
      "    @ Zygote C:\\Users\\heyye\\.julia\\packages\\Zygote\\nsBv0\\src\\compiler\\interface.jl:90\n",
      " [76] pullback\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\Zygote\\nsBv0\\src\\compiler\\interface.jl:88 [inlined]\n",
      " [77] gradient(f::Function, args::ComponentVector{Float32, Vector{Float32}, Tuple{Axis{(layer_1 = ViewAxis(1:256, Axis(weight = ViewAxis(1:192, ShapedAxis((64, 3))), bias = ViewAxis(193:256, ShapedAxis((64, 1))))), layer_2 = ViewAxis(257:8576, Axis(weight = ViewAxis(1:8192, ShapedAxis((128, 64))), bias = ViewAxis(8193:8320, ShapedAxis((128, 1))))), layer_3 = ViewAxis(8577:12704, Axis(weight = ViewAxis(1:4096, ShapedAxis((32, 128))), bias = ViewAxis(4097:4128, ShapedAxis((32, 1))))))}}})\n",
      "    @ Zygote C:\\Users\\heyye\\.julia\\packages\\Zygote\\nsBv0\\src\\compiler\\interface.jl:147\n",
      " [78] (::OptimizationZygoteExt.var\"#38#56\"{OptimizationZygoteExt.var\"#37#55\"{OptimizationFunction{true, AutoZygote, var\"#12#13\", Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED_NO_TIME), Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing}, OptimizationBase.ReInitCache{ComponentVector{Float32, Vector{Float32}, Tuple{Axis{(layer_1 = ViewAxis(1:256, Axis(weight = ViewAxis(1:192, ShapedAxis((64, 3))), bias = ViewAxis(193:256, ShapedAxis((64, 1))))), layer_2 = ViewAxis(257:8576, Axis(weight = ViewAxis(1:8192, ShapedAxis((128, 64))), bias = ViewAxis(8193:8320, ShapedAxis((128, 1))))), layer_3 = ViewAxis(8577:12704, Axis(weight = ViewAxis(1:4096, ShapedAxis((32, 128))), bias = ViewAxis(4097:4128, ShapedAxis((32, 1))))))}}}, SciMLBase.NullParameters}}})(::ComponentVector{Float32, Vector{Float32}, Tuple{Axis{(layer_1 = ViewAxis(1:256, Axis(weight = ViewAxis(1:192, ShapedAxis((64, 3))), bias = ViewAxis(193:256, ShapedAxis((64, 1))))), layer_2 = ViewAxis(257:8576, Axis(weight = ViewAxis(1:8192, ShapedAxis((128, 64))), bias = ViewAxis(8193:8320, ShapedAxis((128, 1))))), layer_3 = ViewAxis(8577:12704, Axis(weight = ViewAxis(1:4096, ShapedAxis((32, 128))), bias = ViewAxis(4097:4128, ShapedAxis((32, 1))))))}}}, ::ComponentVector{Float32, Vector{Float32}, Tuple{Axis{(layer_1 = ViewAxis(1:256, Axis(weight = ViewAxis(1:192, ShapedAxis((64, 3))), bias = ViewAxis(193:256, ShapedAxis((64, 1))))), layer_2 = ViewAxis(257:8576, Axis(weight = ViewAxis(1:8192, ShapedAxis((128, 64))), bias = ViewAxis(8193:8320, ShapedAxis((128, 1))))), layer_3 = ViewAxis(8577:12704, Axis(weight = ViewAxis(1:4096, ShapedAxis((32, 128))), bias = ViewAxis(4097:4128, ShapedAxis((32, 1))))))}}})\n",
      "    @ OptimizationZygoteExt C:\\Users\\heyye\\.julia\\packages\\OptimizationBase\\32Mb0\\ext\\OptimizationZygoteExt.jl:93\n",
      " [79] macro expansion\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\OptimizationOptimisers\\AOkbT\\src\\OptimizationOptimisers.jl:68 [inlined]\n",
      " [80] macro expansion\n",
      "    @ C:\\Users\\heyye\\.julia\\packages\\Optimization\\EmxXu\\src\\utils.jl:32 [inlined]\n",
      " [81] __solve(cache::OptimizationCache{OptimizationFunction{true, AutoZygote, var\"#12#13\", OptimizationZygoteExt.var\"#38#56\"{OptimizationZygoteExt.var\"#37#55\"{OptimizationFunction{true, AutoZygote, var\"#12#13\", Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED_NO_TIME), Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing}, OptimizationBase.ReInitCache{ComponentVector{Float32, Vector{Float32}, Tuple{Axis{(layer_1 = ViewAxis(1:256, Axis(weight = ViewAxis(1:192, ShapedAxis((64, 3))), bias = ViewAxis(193:256, ShapedAxis((64, 1))))), layer_2 = ViewAxis(257:8576, Axis(weight = ViewAxis(1:8192, ShapedAxis((128, 64))), bias = ViewAxis(8193:8320, ShapedAxis((128, 1))))), layer_3 = ViewAxis(8577:12704, Axis(weight = ViewAxis(1:4096, ShapedAxis((32, 128))), bias = ViewAxis(4097:4128, ShapedAxis((32, 1))))))}}}, SciMLBase.NullParameters}}}, OptimizationZygoteExt.var\"#41#59\"{OptimizationZygoteExt.var\"#37#55\"{OptimizationFunction{true, AutoZygote, var\"#12#13\", Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED_NO_TIME), Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing}, OptimizationBase.ReInitCache{ComponentVector{Float32, Vector{Float32}, Tuple{Axis{(layer_1 = ViewAxis(1:256, Axis(weight = ViewAxis(1:192, ShapedAxis((64, 3))), bias = ViewAxis(193:256, ShapedAxis((64, 1))))), layer_2 = ViewAxis(257:8576, Axis(weight = ViewAxis(1:8192, ShapedAxis((128, 64))), bias = ViewAxis(8193:8320, ShapedAxis((128, 1))))), layer_3 = ViewAxis(8577:12704, Axis(weight = ViewAxis(1:4096, ShapedAxis((32, 128))), bias = ViewAxis(4097:4128, ShapedAxis((32, 1))))))}}}, SciMLBase.NullParameters}}}, OptimizationZygoteExt.var\"#45#63\", Nothing, OptimizationZygoteExt.var\"#49#67\"{OptimizationFunction{true, AutoZygote, var\"#12#13\", Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED_NO_TIME), Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing}, OptimizationBase.ReInitCache{ComponentVector{Float32, Vector{Float32}, Tuple{Axis{(layer_1 = ViewAxis(1:256, Axis(weight = ViewAxis(1:192, ShapedAxis((64, 3))), bias = ViewAxis(193:256, ShapedAxis((64, 1))))), layer_2 = ViewAxis(257:8576, Axis(weight = ViewAxis(1:8192, ShapedAxis((128, 64))), bias = ViewAxis(8193:8320, ShapedAxis((128, 1))))), layer_3 = ViewAxis(8577:12704, Axis(weight = ViewAxis(1:4096, ShapedAxis((32, 128))), bias = ViewAxis(4097:4128, ShapedAxis((32, 1))))))}}}, SciMLBase.NullParameters}}, Nothing, Nothing, OptimizationZygoteExt.var\"#53#71\"{OptimizationFunction{true, AutoZygote, var\"#12#13\", Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED_NO_TIME), Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing}, OptimizationBase.ReInitCache{ComponentVector{Float32, Vector{Float32}, Tuple{Axis{(layer_1 = ViewAxis(1:256, Axis(weight = ViewAxis(1:192, ShapedAxis((64, 3))), bias = ViewAxis(193:256, ShapedAxis((64, 1))))), layer_2 = ViewAxis(257:8576, Axis(weight = ViewAxis(1:8192, ShapedAxis((128, 64))), bias = ViewAxis(8193:8320, ShapedAxis((128, 1))))), layer_3 = ViewAxis(8577:12704, Axis(weight = ViewAxis(1:4096, ShapedAxis((32, 128))), bias = ViewAxis(4097:4128, ShapedAxis((32, 1))))))}}}, SciMLBase.NullParameters}}, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED_NO_TIME), Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing}, OptimizationBase.ReInitCache{ComponentVector{Float32, Vector{Float32}, Tuple{Axis{(layer_1 = ViewAxis(1:256, Axis(weight = ViewAxis(1:192, ShapedAxis((64, 3))), bias = ViewAxis(193:256, ShapedAxis((64, 1))))), layer_2 = ViewAxis(257:8576, Axis(weight = ViewAxis(1:8192, ShapedAxis((128, 64))), bias = ViewAxis(8193:8320, ShapedAxis((128, 1))))), layer_3 = ViewAxis(8577:12704, Axis(weight = ViewAxis(1:4096, ShapedAxis((32, 128))), bias = ViewAxis(4097:4128, ShapedAxis((32, 1))))))}}}, SciMLBase.NullParameters}, Nothing, Nothing, Nothing, Nothing, Nothing, Optimisers.Adam, Base.Iterators.Cycle{Tuple{OptimizationBase.NullData}}, Bool, var\"#9#11\"})\n",
      "    @ OptimizationOptimisers C:\\Users\\heyye\\.julia\\packages\\OptimizationOptimisers\\AOkbT\\src\\OptimizationOptimisers.jl:66\n",
      " [82] solve!(cache::OptimizationCache{OptimizationFunction{true, AutoZygote, var\"#12#13\", OptimizationZygoteExt.var\"#38#56\"{OptimizationZygoteExt.var\"#37#55\"{OptimizationFunction{true, AutoZygote, var\"#12#13\", Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED_NO_TIME), Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing}, OptimizationBase.ReInitCache{ComponentVector{Float32, Vector{Float32}, Tuple{Axis{(layer_1 = ViewAxis(1:256, Axis(weight = ViewAxis(1:192, ShapedAxis((64, 3))), bias = ViewAxis(193:256, ShapedAxis((64, 1))))), layer_2 = ViewAxis(257:8576, Axis(weight = ViewAxis(1:8192, ShapedAxis((128, 64))), bias = ViewAxis(8193:8320, ShapedAxis((128, 1))))), layer_3 = ViewAxis(8577:12704, Axis(weight = ViewAxis(1:4096, ShapedAxis((32, 128))), bias = ViewAxis(4097:4128, ShapedAxis((32, 1))))))}}}, SciMLBase.NullParameters}}}, OptimizationZygoteExt.var\"#41#59\"{OptimizationZygoteExt.var\"#37#55\"{OptimizationFunction{true, AutoZygote, var\"#12#13\", Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED_NO_TIME), Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing}, OptimizationBase.ReInitCache{ComponentVector{Float32, Vector{Float32}, Tuple{Axis{(layer_1 = ViewAxis(1:256, Axis(weight = ViewAxis(1:192, ShapedAxis((64, 3))), bias = ViewAxis(193:256, ShapedAxis((64, 1))))), layer_2 = ViewAxis(257:8576, Axis(weight = ViewAxis(1:8192, ShapedAxis((128, 64))), bias = ViewAxis(8193:8320, ShapedAxis((128, 1))))), layer_3 = ViewAxis(8577:12704, Axis(weight = ViewAxis(1:4096, ShapedAxis((32, 128))), bias = ViewAxis(4097:4128, ShapedAxis((32, 1))))))}}}, SciMLBase.NullParameters}}}, OptimizationZygoteExt.var\"#45#63\", Nothing, OptimizationZygoteExt.var\"#49#67\"{OptimizationFunction{true, AutoZygote, var\"#12#13\", Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED_NO_TIME), Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing}, OptimizationBase.ReInitCache{ComponentVector{Float32, Vector{Float32}, Tuple{Axis{(layer_1 = ViewAxis(1:256, Axis(weight = ViewAxis(1:192, ShapedAxis((64, 3))), bias = ViewAxis(193:256, ShapedAxis((64, 1))))), layer_2 = ViewAxis(257:8576, Axis(weight = ViewAxis(1:8192, ShapedAxis((128, 64))), bias = ViewAxis(8193:8320, ShapedAxis((128, 1))))), layer_3 = ViewAxis(8577:12704, Axis(weight = ViewAxis(1:4096, ShapedAxis((32, 128))), bias = ViewAxis(4097:4128, ShapedAxis((32, 1))))))}}}, SciMLBase.NullParameters}}, Nothing, Nothing, OptimizationZygoteExt.var\"#53#71\"{OptimizationFunction{true, AutoZygote, var\"#12#13\", Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED_NO_TIME), Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing}, OptimizationBase.ReInitCache{ComponentVector{Float32, Vector{Float32}, Tuple{Axis{(layer_1 = ViewAxis(1:256, Axis(weight = ViewAxis(1:192, ShapedAxis((64, 3))), bias = ViewAxis(193:256, ShapedAxis((64, 1))))), layer_2 = ViewAxis(257:8576, Axis(weight = ViewAxis(1:8192, ShapedAxis((128, 64))), bias = ViewAxis(8193:8320, ShapedAxis((128, 1))))), layer_3 = ViewAxis(8577:12704, Axis(weight = ViewAxis(1:4096, ShapedAxis((32, 128))), bias = ViewAxis(4097:4128, ShapedAxis((32, 1))))))}}}, SciMLBase.NullParameters}}, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED_NO_TIME), Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing}, OptimizationBase.ReInitCache{ComponentVector{Float32, Vector{Float32}, Tuple{Axis{(layer_1 = ViewAxis(1:256, Axis(weight = ViewAxis(1:192, ShapedAxis((64, 3))), bias = ViewAxis(193:256, ShapedAxis((64, 1))))), layer_2 = ViewAxis(257:8576, Axis(weight = ViewAxis(1:8192, ShapedAxis((128, 64))), bias = ViewAxis(8193:8320, ShapedAxis((128, 1))))), layer_3 = ViewAxis(8577:12704, Axis(weight = ViewAxis(1:4096, ShapedAxis((32, 128))), bias = ViewAxis(4097:4128, ShapedAxis((32, 1))))))}}}, SciMLBase.NullParameters}, Nothing, Nothing, Nothing, Nothing, Nothing, Optimisers.Adam, Base.Iterators.Cycle{Tuple{OptimizationBase.NullData}}, Bool, var\"#9#11\"})\n",
      "    @ SciMLBase C:\\Users\\heyye\\.julia\\packages\\SciMLBase\\rR75x\\src\\solve.jl:188\n",
      " [83] solve(::OptimizationProblem{true, OptimizationFunction{true, AutoZygote, var\"#12#13\", Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED_NO_TIME), Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing}, ComponentVector{Float32, Vector{Float32}, Tuple{Axis{(layer_1 = ViewAxis(1:256, Axis(weight = ViewAxis(1:192, ShapedAxis((64, 3))), bias = ViewAxis(193:256, ShapedAxis((64, 1))))), layer_2 = ViewAxis(257:8576, Axis(weight = ViewAxis(1:8192, ShapedAxis((128, 64))), bias = ViewAxis(8193:8320, ShapedAxis((128, 1))))), layer_3 = ViewAxis(8577:12704, Axis(weight = ViewAxis(1:4096, ShapedAxis((32, 128))), bias = ViewAxis(4097:4128, ShapedAxis((32, 1))))))}}}, SciMLBase.NullParameters, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, @Kwargs{}}, ::Optimisers.Adam; kwargs::@Kwargs{callback::var\"#9#11\", maxiters::Int64})\n",
      "    @ SciMLBase C:\\Users\\heyye\\.julia\\packages\\SciMLBase\\rR75x\\src\\solve.jl:96"
     ]
    }
   ],
   "source": [
    "# Train using the Adam optimizer\n",
    "adtype = Optimization.AutoZygote()\n",
    "\n",
    "optf = Optimization.OptimizationFunction((x, p) -> loss_neuralode(x), adtype)\n",
    "optprob = Optimization.OptimizationProblem(optf, pinit)\n",
    "\n",
    "result_neuralode = Optimization.solve(\n",
    "    optprob, OptimizationOptimisers.Adam(0.05); callback = callback, maxiters = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `result_neuralode` not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `result_neuralode` not defined\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ g:\\My Drive\\Ga2O3\\DiffEqFlux\\AM-NODE.ipynb:2"
     ]
    }
   ],
   "source": [
    "# Retrain using the LBFGS optimizer\n",
    "optprob2 = remake(optprob; u0 = result_neuralode.u)\n",
    "\n",
    "result_neuralode2 = Optimization.solve(optprob2, Optim.BFGS(; initial_stepnorm = 0.01);\n",
    "    callback = callback, allow_f_increases = false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `result_neuralode2` not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `result_neuralode2` not defined\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ g:\\My Drive\\Ga2O3\\DiffEqFlux\\AM-NODE.ipynb:1"
     ]
    }
   ],
   "source": [
    "callback(result_neuralode2.u, loss_neuralode(result_neuralode2.u)...; doplot = true)\n",
    "scatter!(plt, tsteps, loss_neuralode(result_neuralode2.u)[2][1, :]; label = \"prediction\") # hide"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.1",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
